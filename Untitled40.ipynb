{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1VYwFWA8uTgvXwYjWLo0zA8Hb-IdEumvB",
      "authorship_tag": "ABX9TyOJqI732XwjryfrR0+RZdSi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eaa94c4644744a0fb27375c3f6921409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e70beb07d2d4bdf8460b420ac21f89c",
              "IPY_MODEL_ceb1e0dc88a94e0bb55c6d3b4bfb80fc",
              "IPY_MODEL_d6c8128479af45269760bddfee1d199d"
            ],
            "layout": "IPY_MODEL_8a631d8bb328447d83ed3f0c1dabf4fb"
          }
        },
        "7e70beb07d2d4bdf8460b420ac21f89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c00269c32094dc489f53593112024b7",
            "placeholder": "​",
            "style": "IPY_MODEL_7101102492864f7b87021e2b28dd9dfc",
            "value": "model.safetensors: 100%"
          }
        },
        "ceb1e0dc88a94e0bb55c6d3b4bfb80fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb8c891bc2fe415fbd920a0e4e8e5845",
            "max": 86523256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_003e5f28eef64ff0bc22a7ad2b7c18a9",
            "value": 86523256
          }
        },
        "d6c8128479af45269760bddfee1d199d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f861d67967cb41238b94bd2db97596e7",
            "placeholder": "​",
            "style": "IPY_MODEL_2c553b9e86614629a6e4227513a03bf9",
            "value": " 86.5M/86.5M [00:02&lt;00:00, 12.1MB/s]"
          }
        },
        "8a631d8bb328447d83ed3f0c1dabf4fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c00269c32094dc489f53593112024b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7101102492864f7b87021e2b28dd9dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb8c891bc2fe415fbd920a0e4e8e5845": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "003e5f28eef64ff0bc22a7ad2b7c18a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f861d67967cb41238b94bd2db97596e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c553b9e86614629a6e4227513a03bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotaro-desu/google-colab/blob/main/Untitled40.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w6GR36g8ZrE",
        "outputId": "1e84bb74-752b-4d1c-c424-f5125627a5e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "EfficientNetV2-S Multi-Output Binary Classification with Optuna (Dataset v5 - Fixed Resolution)\n",
        "Predicts rainfall presence (Rain/Clear) for 9 regions (3x3 grid) simultaneously.\n",
        "\n",
        "[V5 Changes Applied]\n",
        "- Fixed Resolution: 300px (Progressive learning removed)\n",
        "- LR Scheduler: CosineAnnealingLR or OneCycleLR\n",
        "- Updated Hyperparameters: Dropout, Augment Magnitude, Freeze Ratio (0.0-1.0)\n",
        "- Epoch Monitor: Warns if too many trials reach max epochs.\n",
        "- Robust Integrity Check & Caching preserved.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "import argparse\n",
        "import json\n",
        "from PIL import Image\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import shutil\n",
        "import random\n",
        "import os\n",
        "import sqlite3\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# --- GLOBAL CONFIG (V5) ---\n",
        "LOCAL_CACHE_DIR = Path('/content/temp_dataset_cache')\n",
        "RESOLUTION = 300\n",
        "MAX_EPOCHS = 100\n",
        "PATIENCE = 5\n",
        "EPOCH_MONITOR_START = 15\n",
        "EPOCH_MONITOR_THRESHOLD = 0.5\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print(f\"Global seed set to: {seed}\")\n",
        "\n",
        "# --- V5: AUGMENTATION (Dynamic Magnitude) ---\n",
        "def get_transforms(augment=True, magnitude=10):\n",
        "    if augment:\n",
        "        return transforms.Compose([\n",
        "            transforms.RandAugment(num_ops=2, magnitude=magnitude),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "# --- HELPER: FREEZE LAYERS ---\n",
        "def freeze_layers(model, freeze_ratio=0.0):\n",
        "    if freeze_ratio <= 0.0:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "        return\n",
        "\n",
        "    base = model.base_model\n",
        "    conv_layers = []\n",
        "    for name, module in base.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            conv_layers.append((name, module))\n",
        "\n",
        "    num_layers = len(conv_layers)\n",
        "    num_freeze = int(num_layers * freeze_ratio)\n",
        "\n",
        "    frozen_names = set()\n",
        "    for name, module in conv_layers[:num_freeze]:\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False\n",
        "        frozen_names.add(name.split('.')[0])\n",
        "\n",
        "    for name, param in base.named_parameters():\n",
        "        block_name = name.split('.')[0]\n",
        "        if block_name in frozen_names:\n",
        "            param.requires_grad = False\n",
        "\n",
        "# --- INTEGRITY CHECK ---\n",
        "def check_pickle_integrity(path):\n",
        "    if not path.exists():\n",
        "        return False\n",
        "    try:\n",
        "        with open(path, 'rb') as f:\n",
        "            _ = pickle.load(f)\n",
        "        return True\n",
        "    except (EOFError, pickle.UnpicklingError, Exception) as e:\n",
        "        print(f\"    [Corrupt] File is broken: {path} ({e})\")\n",
        "        return False\n",
        "\n",
        "# --- DATA LOADING FUNCTIONS ---\n",
        "def get_file_list(base_dir, split, resolution):\n",
        "    data_dir = Path(base_dir) / 'npz_datasets_v4' / f'npz_datasets_v4_universal_{resolution}px'\n",
        "    if split == 'train':\n",
        "        patterns = ['train_rain_*.npz', 'train_clear_*.npz']\n",
        "    elif split == 'val_balanced':\n",
        "        patterns = ['val_balanced_rain_*.npz', 'val_balanced_clear_*.npz']\n",
        "    else:\n",
        "        patterns = [f'{split}_*.npz']\n",
        "    files = []\n",
        "    for pat in patterns:\n",
        "        files.extend(sorted(data_dir.glob(pat)))\n",
        "    return files\n",
        "\n",
        "def create_subset_two_pass(resolution, org_name, base_dir, split, target_size='3x', seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    files = get_file_list(base_dir, split, resolution)\n",
        "    if not files:\n",
        "        print(f\"      [WARNING] No files found for {split} {resolution}px\")\n",
        "        return None, None\n",
        "\n",
        "    # --- PASS 1: SCAN METADATA ---\n",
        "    print(f\"      [Pass 1] Scanning {split} labels...\")\n",
        "    file_map = []\n",
        "    all_univ_levels = []\n",
        "    all_org_labels = []\n",
        "    key_error_reported = False\n",
        "\n",
        "    for f_idx, chunk_file in enumerate(tqdm(files, desc=\"Scanning Labels\", leave=False)):\n",
        "        try:\n",
        "            with np.load(chunk_file, allow_pickle=True) as data:\n",
        "                levels_dict = data['levels'].item()\n",
        "                if org_name not in levels_dict:\n",
        "                    if not key_error_reported:\n",
        "                        print(f\"      [SKIP] Key '{org_name}' missing\")\n",
        "                        key_error_reported = True\n",
        "                    continue\n",
        "                lbls = levels_dict[org_name]\n",
        "                univ = levels_dict['Universal']\n",
        "                file_map.append({'path': chunk_file, 'count': len(lbls)})\n",
        "                all_org_labels.append(lbls)\n",
        "                all_univ_levels.append(univ)\n",
        "        except Exception as e:\n",
        "            print(f\"      [ERROR] reading {chunk_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_org_labels: return None, None\n",
        "\n",
        "    full_org_labels = np.concatenate(all_org_labels, axis=0)\n",
        "    full_univ_levels = np.concatenate(all_univ_levels, axis=0)\n",
        "    total_samples = len(full_org_labels)\n",
        "\n",
        "    base_count = 10000\n",
        "    if target_size == '1x': target_count = base_count * 2\n",
        "    elif target_size == '2x': target_count = base_count * 4\n",
        "    elif target_size == '3x': target_count = base_count * 6\n",
        "    else: target_count = base_count * 2\n",
        "\n",
        "    if split == 'val_balanced': target_count = 20000\n",
        "    if split == 'val_imbalanced': target_count = total_samples\n",
        "\n",
        "    if split == 'val_imbalanced':\n",
        "        if total_samples > target_count:\n",
        "            selected_global_indices = np.random.choice(total_samples, target_count, replace=False)\n",
        "        else:\n",
        "            selected_global_indices = np.arange(total_samples)\n",
        "    else:\n",
        "        target_per_class = target_count // 2\n",
        "        max_univ = full_univ_levels.max(axis=1)\n",
        "        rain_indices = np.where(max_univ > 0)[0]\n",
        "        clear_indices = np.where(max_univ == 0)[0]\n",
        "\n",
        "        if len(rain_indices) >= target_per_class:\n",
        "            sel_rain = np.random.choice(rain_indices, target_per_class, replace=False)\n",
        "        else:\n",
        "            sel_rain = np.concatenate([rain_indices, np.random.choice(rain_indices, target_per_class - len(rain_indices), replace=True)])\n",
        "\n",
        "        if len(clear_indices) >= target_per_class:\n",
        "            sel_clear = np.random.choice(clear_indices, target_per_class, replace=False)\n",
        "        else:\n",
        "            sel_clear = np.concatenate([clear_indices, np.random.choice(clear_indices, target_per_class - len(clear_indices), replace=True)])\n",
        "\n",
        "        selected_global_indices = np.concatenate([sel_rain, sel_clear])\n",
        "\n",
        "    np.random.shuffle(selected_global_indices)\n",
        "    final_count = len(selected_global_indices)\n",
        "    print(f\"      Selected {final_count} samples (Target: {target_size})\")\n",
        "\n",
        "    del full_univ_levels, all_univ_levels, all_org_labels\n",
        "    gc.collect()\n",
        "\n",
        "    # --- PASS 2: EXTRACT IMAGES ---\n",
        "    print(f\"      [Pass 2] Extracting images...\")\n",
        "    files_to_load = {}\n",
        "    sorted_indices = np.sort(selected_global_indices)\n",
        "\n",
        "    current_file_idx = 0\n",
        "    current_file_start = 0\n",
        "    current_file_end = file_map[0]['count']\n",
        "\n",
        "    for global_idx in sorted_indices:\n",
        "        while global_idx >= current_file_end:\n",
        "            current_file_idx += 1\n",
        "            if current_file_idx >= len(file_map): break\n",
        "            current_file_start = current_file_end\n",
        "            current_file_end += file_map[current_file_idx]['count']\n",
        "\n",
        "        if current_file_idx >= len(file_map): break\n",
        "        local_idx = global_idx - current_file_start\n",
        "        f_path = file_map[current_file_idx]['path']\n",
        "        if f_path not in files_to_load: files_to_load[f_path] = []\n",
        "        files_to_load[f_path].append(local_idx)\n",
        "\n",
        "    sample_path = list(files_to_load.keys())[0]\n",
        "    with np.load(sample_path, allow_pickle=True) as data:\n",
        "        img_shape = data['images'][0].shape\n",
        "\n",
        "    print(f\"      Allocating memory for {final_count} images of shape {img_shape}...\")\n",
        "    final_images_arr = np.empty((final_count, *img_shape), dtype=np.uint8)\n",
        "    final_labels_arr = np.empty((final_count, 9), dtype=np.int64)\n",
        "\n",
        "    current_fill_idx = 0\n",
        "    for f_path, local_indices in tqdm(files_to_load.items(), desc=\"Extracting\", leave=False):\n",
        "        try:\n",
        "            with np.load(f_path, allow_pickle=True) as data:\n",
        "                raw_imgs = data['images']\n",
        "                levels_dict = data['levels'].item()\n",
        "                raw_lbls = levels_dict[org_name]\n",
        "                idx_arr = np.array(local_indices)\n",
        "                batch_size = len(idx_arr)\n",
        "                final_images_arr[current_fill_idx : current_fill_idx + batch_size] = raw_imgs[idx_arr]\n",
        "                final_labels_arr[current_fill_idx : current_fill_idx + batch_size] = raw_lbls[idx_arr]\n",
        "                current_fill_idx += batch_size\n",
        "        except Exception: continue\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    shuffle_idx = np.arange(final_count)\n",
        "    np.random.shuffle(shuffle_idx)\n",
        "    final_images_arr = final_images_arr[shuffle_idx]\n",
        "    final_labels_arr = final_labels_arr[shuffle_idx]\n",
        "\n",
        "    gc.collect()\n",
        "    return final_images_arr, final_labels_arr\n",
        "\n",
        "def prepare_and_cache_split(split, resolution, org_name, base_dir, drive_cache_dir, seed, target_size=None):\n",
        "    file_name = f\"{split}_{resolution}.pkl\"\n",
        "    local_path = LOCAL_CACHE_DIR / file_name\n",
        "    drive_path = drive_cache_dir / file_name\n",
        "\n",
        "    if local_path.exists():\n",
        "        if check_pickle_integrity(local_path):\n",
        "            print(f\"    [Found] Local cache for {split} {resolution}px\")\n",
        "            return\n",
        "        else:\n",
        "            local_path.unlink()\n",
        "\n",
        "    if drive_path.exists():\n",
        "        print(f\"    [Found] Drive cache for {split} {resolution}px. Copying...\")\n",
        "        try:\n",
        "            shutil.copy(drive_path, local_path)\n",
        "            if check_pickle_integrity(local_path): return\n",
        "            else: local_path.unlink()\n",
        "        except Exception: pass\n",
        "\n",
        "    print(f\"    [Create] Generating {split} {resolution}px data...\")\n",
        "    imgs, lbls = create_subset_two_pass(resolution, org_name, base_dir, split, target_size=target_size, seed=seed)\n",
        "\n",
        "    if imgs is not None:\n",
        "        with open(local_path, 'wb') as f:\n",
        "            pickle.dump({'images': imgs, 'labels': lbls}, f, protocol=4)\n",
        "        try:\n",
        "            shutil.copy(local_path, drive_path)\n",
        "        except Exception: pass\n",
        "        del imgs, lbls\n",
        "        gc.collect()\n",
        "\n",
        "def prepare_all_resolutions(org_name, base_dir, seed=42):\n",
        "    \"\"\"V5: Prepares ONLY the fixed resolution (300px).\"\"\"\n",
        "    print(f\"\\n{'='*20} PRE-CACHING DATASET (V5 Fixed Res, Seed={seed}) {'='*20}\")\n",
        "    LOCAL_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    drive_cache_dir = Path(base_dir) / 'dataset_cache'\n",
        "    drive_cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    res = RESOLUTION\n",
        "    print(f\"\\n>> Checking Resolution: {res}px\")\n",
        "    prepare_and_cache_split('train', res, org_name, base_dir, drive_cache_dir, seed, target_size='3x')\n",
        "    prepare_and_cache_split('val_balanced', res, org_name, base_dir, drive_cache_dir, seed, target_size='1x')\n",
        "    prepare_and_cache_split('val_imbalanced', res, org_name, base_dir, drive_cache_dir, seed)\n",
        "    print(f\"\\n{'='*20} CACHING CHECK COMPLETE {'='*20}\\n\")\n",
        "\n",
        "class CachedBinaryDataset(Dataset):\n",
        "    def __init__(self, split, resolution, transform=None, dataset_size='1x', seed=42):\n",
        "        self.transform = transform\n",
        "        self.seed = seed\n",
        "        cache_path = LOCAL_CACHE_DIR / f\"{split}_{resolution}.pkl\"\n",
        "        if not cache_path.exists():\n",
        "            raise FileNotFoundError(f\"Cache not found: {cache_path}\")\n",
        "        with open(cache_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.images = data['images']\n",
        "        self.labels = data['labels']\n",
        "        if split == 'train' and dataset_size != '3x':\n",
        "            self._downsample_from_cache(dataset_size)\n",
        "        # Binary Classification: > 0 is Rain (1), else Clear (0)\n",
        "        self.labels_binary = (self.labels > 0).astype(np.int64)\n",
        "\n",
        "    def _downsample_from_cache(self, target_size):\n",
        "        np.random.seed(self.seed)\n",
        "        total_available = len(self.images)\n",
        "        target_count = 20000 if target_size == '1x' else 40000 if target_size == '2x' else None\n",
        "        if target_count and target_count < total_available:\n",
        "            indices = np.random.permutation(total_available)[:target_count]\n",
        "            self.images = self.images[indices]\n",
        "            self.labels = self.labels[indices]\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.fromarray(self.images[idx])\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, torch.from_numpy(self.labels_binary[idx]).long()\n",
        "\n",
        "class MultiOutputEfficientNetBinary(nn.Module):\n",
        "    def __init__(self, model_name, dropout=0.0, pretrained=True,\n",
        "                 head_layers=1, head_hidden_dim=512, head_dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.base_model = timm.create_model(model_name, pretrained=pretrained, drop_rate=dropout, num_classes=0)\n",
        "        in_features = self.base_model.num_features\n",
        "\n",
        "        if head_layers == 1:\n",
        "            self.head = nn.Linear(in_features, 9 * 2)\n",
        "        elif head_layers == 2:\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(in_features, head_hidden_dim),\n",
        "                nn.BatchNorm1d(head_hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Dropout(head_dropout),\n",
        "                nn.Linear(head_hidden_dim, 9 * 2)\n",
        "            )\n",
        "        elif head_layers == 3:\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(in_features, head_hidden_dim),\n",
        "                nn.BatchNorm1d(head_hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Dropout(head_dropout),\n",
        "                nn.Linear(head_hidden_dim, head_hidden_dim),\n",
        "                nn.BatchNorm1d(head_hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Dropout(head_dropout),\n",
        "                nn.Linear(head_hidden_dim, 9 * 2)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.base_model(x)\n",
        "        logits = self.head(features)\n",
        "        return logits.view(-1, 9, 2)\n",
        "\n",
        "# --- V5: EPOCH MONITOR CALLBACK ---\n",
        "class EpochMonitorCallback:\n",
        "    def __init__(self, start_trial=EPOCH_MONITOR_START, threshold=EPOCH_MONITOR_THRESHOLD):\n",
        "        self.start_trial = start_trial\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def __call__(self, study, trial):\n",
        "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "        if len(completed_trials) < self.start_trial:\n",
        "            return\n",
        "\n",
        "        max_epoch_count = 0\n",
        "        for t in completed_trials:\n",
        "            if 'final_epoch' in t.user_attrs and t.user_attrs['final_epoch'] >= MAX_EPOCHS:\n",
        "                max_epoch_count += 1\n",
        "\n",
        "        ratio = max_epoch_count / len(completed_trials)\n",
        "        if ratio > self.threshold:\n",
        "            print(f\"\\n[WARNING] {ratio*100:.1f}% of trials reached max epochs! Consider increasing MAX_EPOCHS.\")\n",
        "\n",
        "# --- V5: TRAIN FUNCTION (With Scheduler) ---\n",
        "def train_one_epoch(model, loader, criterion, optimizer, scheduler, scaler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # OneCycleLR steps per batch\n",
        "        if scheduler is not None and isinstance(scheduler, optim.lr_scheduler.OneCycleLR):\n",
        "            scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(outputs, dim=2)[:, :, 1]\n",
        "            preds = (probs > 0.5).long()\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds).flatten()\n",
        "    all_targets = np.concatenate(all_targets).flatten()\n",
        "    acc = (all_preds == all_targets).mean() * 100.0\n",
        "    return running_loss / len(loader), acc\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_probs = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
        "            running_loss += loss.item()\n",
        "            probs = torch.softmax(outputs, dim=2)[:, :, 1]\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_targets.append(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.concatenate(all_probs).flatten()\n",
        "    all_targets = np.concatenate(all_targets).flatten()\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_thresh = 0.5\n",
        "    best_metrics = {}\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "    for th in thresholds:\n",
        "        preds = (all_probs > th).astype(int)\n",
        "        f1 = f1_score(all_targets, preds, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_thresh = th\n",
        "            best_metrics = {\n",
        "                'acc': (preds == all_targets).mean() * 100.0,\n",
        "                'precision': precision_score(all_targets, preds, zero_division=0),\n",
        "                'recall': recall_score(all_targets, preds, zero_division=0)\n",
        "            }\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(all_targets, all_probs)\n",
        "        pr_auc = average_precision_score(all_targets, all_probs)\n",
        "    except: roc_auc, pr_auc = 0.0, 0.0\n",
        "\n",
        "    return avg_loss, best_metrics.get('acc', 0), best_metrics.get('precision', 0), \\\n",
        "           best_metrics.get('recall', 0), best_f1, roc_auc, pr_auc, best_thresh\n",
        "\n",
        "# --- V5: OPTUNA OBJECTIVE ---\n",
        "def objective(trial, device, seed):\n",
        "    # --- V5 Hyperparameters ---\n",
        "    dataset_size = trial.suggest_categorical('dataset_size', ['1x', '2x', '3x'])\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [64, 128])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # New V5 Params\n",
        "    dropout = trial.suggest_float('dropout', 0.1, 0.4)\n",
        "    freeze_ratio = trial.suggest_float('freeze_ratio', 0.0, 1.0)\n",
        "    augment_magnitude = trial.suggest_int('augment_magnitude', 5, 15)\n",
        "    scheduler_type = trial.suggest_categorical('scheduler', ['cosine', 'onecycle'])\n",
        "\n",
        "    head_layers = trial.suggest_int('head_layers', 1, 3)\n",
        "    head_hidden_dim = trial.suggest_categorical('head_hidden_dim', [512, 768, 1024, 1280])\n",
        "    head_dropout = trial.suggest_float('head_dropout', 0.1, 0.5)\n",
        "\n",
        "    print(f\"\\nTrial {trial.number}: size={dataset_size}, lr={lr:.2e}, batch={batch_size}, opt={optimizer_name}, \"\n",
        "          f\"sched={scheduler_type}, mag={augment_magnitude}, drop={dropout:.2f}, freeze={freeze_ratio:.2f}\")\n",
        "\n",
        "    # Initialize Model with Fixed Resolution V5 Config\n",
        "    model = MultiOutputEfficientNetBinary('tf_efficientnetv2_s', dropout=dropout, pretrained=True,\n",
        "                                          head_layers=head_layers, head_hidden_dim=head_hidden_dim,\n",
        "                                          head_dropout=head_dropout).to(device)\n",
        "    freeze_layers(model, freeze_ratio)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if optimizer_name == 'Adam': optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW': optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else: optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "    # Compile for speed (if available)\n",
        "    try:\n",
        "        model = torch.compile(model, mode='reduce-overhead')\n",
        "    except Exception: pass\n",
        "\n",
        "    try:\n",
        "        train_ds = CachedBinaryDataset('train', RESOLUTION, get_transforms(True, augment_magnitude), dataset_size, seed)\n",
        "        val_bal_ds = CachedBinaryDataset('val_balanced', RESOLUTION, get_transforms(False), seed=seed)\n",
        "        val_imbal_ds = CachedBinaryDataset('val_imbalanced', RESOLUTION, get_transforms(False), seed=seed)\n",
        "    except FileNotFoundError: raise optuna.TrialPruned()\n",
        "\n",
        "    num_workers = 4\n",
        "    train_loader = DataLoader(train_ds, batch_size, True, num_workers=num_workers, pin_memory=True, persistent_workers=True)\n",
        "    val_bal_loader = DataLoader(val_bal_ds, batch_size*2, False, num_workers=num_workers, pin_memory=True)\n",
        "    val_imbal_loader = DataLoader(val_imbal_ds, batch_size*2, False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    # Scheduler Setup\n",
        "    if scheduler_type == 'cosine':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS, eta_min=1e-6)\n",
        "    else:\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=MAX_EPOCHS, steps_per_epoch=len(train_loader))\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    final_epoch = 0\n",
        "\n",
        "    # Single Loop Training (V5)\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        final_epoch = epoch + 1\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, device)\n",
        "\n",
        "        # Step scheduler (Cosine only, OneCycle steps per batch)\n",
        "        if scheduler is not None and not isinstance(scheduler, optim.lr_scheduler.OneCycleLR):\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        vb_metrics = validate(model, val_bal_loader, criterion, device)\n",
        "        # vi_metrics = validate(model, val_imbal_loader, criterion, device) # Optional to save time\n",
        "\n",
        "        vb_loss, vb_acc, vb_prec, vb_rec, vb_f1, vb_auc, vb_pr, vb_th = vb_metrics\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}/{MAX_EPOCHS}: Loss:{train_loss:.4f} | LR:{current_lr:.2e} | ValF1:{vb_f1:.4f}(@{vb_th:.2f})\")\n",
        "\n",
        "        trial.report(vb_f1, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        if vb_f1 > best_val_f1:\n",
        "            best_val_f1 = vb_f1\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    trial.set_user_attr('final_epoch', final_epoch)\n",
        "\n",
        "    del train_ds, val_bal_ds, val_imbal_ds, train_loader, val_bal_loader, val_imbal_loader\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    return best_val_f1\n",
        "\n",
        "def main(argv=None):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--org', type=str, required=True)\n",
        "    parser.add_argument('--n_trials', type=int, default=100)\n",
        "    parser.add_argument('--study_name', type=str, default=None)\n",
        "    parser.add_argument('--base_dir', type=str, default='/content/drive/MyDrive/XRAIN/yano/20250601~20251020_dataset/')\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "\n",
        "    if argv is not None: args = parser.parse_args(argv)\n",
        "    else: args = parser.parse_args()\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    study_name = args.study_name or f'effv2_s_{args.org}_v5_binary_fixed'\n",
        "    print(f\"Study: {study_name}, Seed: {args.seed}, Org: {args.org}, Res: {RESOLUTION}px\")\n",
        "\n",
        "    # 1. Prepare Cache (V5: Fixed 300px)\n",
        "    prepare_all_resolutions(args.org, args.base_dir, seed=args.seed)\n",
        "\n",
        "    # 2. Run Optuna\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    output_dir = Path(args.base_dir) / 'output'\n",
        "    progress_dir = output_dir / 'progress' / 'binary_v5'\n",
        "    progress_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    db_path = progress_dir / f\"{study_name}.db\"\n",
        "    storage_url = f\"sqlite:///{db_path}\"\n",
        "\n",
        "    try:\n",
        "        study = optuna.create_study(\n",
        "            study_name=study_name,\n",
        "            storage=storage_url,\n",
        "            load_if_exists=True,\n",
        "            direction='maximize',\n",
        "            sampler=TPESampler(seed=args.seed),\n",
        "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
        "        )\n",
        "    except Exception:\n",
        "        if db_path.exists(): shutil.move(str(db_path), str(db_path.with_suffix('.db.bak')))\n",
        "        study = optuna.create_study(\n",
        "            study_name=study_name,\n",
        "            storage=storage_url,\n",
        "            direction='maximize',\n",
        "            sampler=TPESampler(seed=args.seed),\n",
        "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
        "        )\n",
        "\n",
        "    remaining = args.n_trials - len(study.trials)\n",
        "    if remaining > 0:\n",
        "        study.optimize(lambda t: objective(t, device, args.seed),\n",
        "                       n_trials=remaining,\n",
        "                       callbacks=[EpochMonitorCallback()],\n",
        "                       catch=(RuntimeError,))\n",
        "\n",
        "    if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 0:\n",
        "        print(f\"Best F1: {study.best_value:.4f}\")\n",
        "        print(f\"Best params: {study.best_params}\")\n",
        "        with open(output_dir / f'optuna_results_{args.org}_v5_binary.json', 'w') as f:\n",
        "            json.dump({\n",
        "                'org': args.org,\n",
        "                'best_trial': study.best_trial.number,\n",
        "                'best_f1': study.best_value,\n",
        "                'best_params': study.best_params,\n",
        "                'seed': args.seed\n",
        "            }, f, indent=2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        main(['--org', 'JMA', '--seed', '42'])\n",
        "    else:\n",
        "        main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eaa94c4644744a0fb27375c3f6921409",
            "7e70beb07d2d4bdf8460b420ac21f89c",
            "ceb1e0dc88a94e0bb55c6d3b4bfb80fc",
            "d6c8128479af45269760bddfee1d199d",
            "8a631d8bb328447d83ed3f0c1dabf4fb",
            "2c00269c32094dc489f53593112024b7",
            "7101102492864f7b87021e2b28dd9dfc",
            "cb8c891bc2fe415fbd920a0e4e8e5845",
            "003e5f28eef64ff0bc22a7ad2b7c18a9",
            "f861d67967cb41238b94bd2db97596e7",
            "2c553b9e86614629a6e4227513a03bf9"
          ]
        },
        "id": "t8-mUD1q6636",
        "outputId": "8b7d1371-881c-4b0e-96cd-f000251d7407"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed set to: 42\n",
            "Study: effv2_s_JMA_v5_binary_fixed, Seed: 42, Org: JMA, Res: 300px\n",
            "\n",
            "==================== PRE-CACHING DATASET (V5 Fixed Res, Seed=42) ====================\n",
            "\n",
            ">> Checking Resolution: 300px\n",
            "    [Found] Drive cache for train 300px. Copying...\n",
            "    [Found] Drive cache for val_balanced 300px. Copying...\n",
            "    [Found] Drive cache for val_imbalanced 300px. Copying...\n",
            "\n",
            "==================== CACHING CHECK COMPLETE ====================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-29 01:43:15,007] Using an existing study with name 'effv2_s_JMA_v5_binary_fixed' instead of creating a new one.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 74: size=3x, lr=7.15e-04, batch=128, opt=Adam, sched=cosine, mag=6, drop=0.31, freeze=0.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaa94c4644744a0fb27375c3f6921409"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4070541459.py:527: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Training:   0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "Training:   0%|          | 1/469 [01:58<15:25:34, 118.66s/it]/tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training:   0%|          | 2/469 [01:59<6:24:37, 49.42s/it]  /tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Validation:   0%|          | 0/79 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:435: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Validation:   1%|▏         | 1/79 [00:48<1:03:04, 48.52s/it]/tmp/ipython-input-4070541459.py:435: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Validation:   3%|▎         | 2/79 [00:48<25:45, 20.07s/it]  /tmp/ipython-input-4070541459.py:435: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100: Loss:0.4504 | LR:7.15e-04 | ValF1:0.7563(@0.45)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training:   0%|          | 2/469 [00:06<22:35,  2.90s/it]/tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Validation:   0%|          | 0/79 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:435: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100: Loss:0.4202 | LR:7.14e-04 | ValF1:0.7628(@0.35)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Validation:   0%|          | 0/79 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:435: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100: Loss:0.4091 | LR:7.13e-04 | ValF1:0.7632(@0.45)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Validation:   0%|          | 0/79 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:435: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100: Loss:0.4031 | LR:7.12e-04 | ValF1:0.7678(@0.40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipython-input-4070541459.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "[W 2025-11-29 01:56:56,224] Trial 74 failed with parameters: {'dataset_size': '3x', 'lr': 0.000714803094926685, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 3.5300280258141837e-06, 'dropout': 0.309553345371676, 'freeze_ratio': 0.6645870472609519, 'augment_magnitude': 6, 'scheduler': 'cosine', 'head_layers': 3, 'head_hidden_dim': 1280, 'head_dropout': 0.17765512344617737} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4070541459.py\", line 620, in <lambda>\n",
            "    study.optimize(lambda t: objective(t, device, args.seed),\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4070541459.py\", line 536, in objective\n",
            "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, device)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4070541459.py\", line 407, in train_one_epoch\n",
            "    scaler.step(optimizer)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\", line 462, in step\n",
            "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\", line 356, in _maybe_opt_step\n",
            "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\", line 356, in <genexpr>\n",
            "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
            "               ^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-11-29 01:56:56,227] Trial 74 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4070541459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'ipykernel'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'--org'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JMA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--seed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'42'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4070541459.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         study.optimize(lambda t: objective(t, device, args.seed),\n\u001b[0m\u001b[1;32m    621\u001b[0m                        \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEpochMonitorCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     ):\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4070541459.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         study.optimize(lambda t: objective(t, device, args.seed),\n\u001b[0m\u001b[1;32m    621\u001b[0m                        \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEpochMonitorCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4070541459.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, device, seed)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mfinal_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# Step scheduler (Cosine only, OneCycle steps per batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4070541459.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, criterion, optimizer, scheduler, scaler, device)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    355\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    355\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}